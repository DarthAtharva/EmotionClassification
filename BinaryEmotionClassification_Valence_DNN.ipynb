{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DarthAtharva/EmotionClassification/blob/main/BinaryEmotionClassification_Valence_DNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#IMPORTS"
      ],
      "metadata": {
        "id": "Ef1g1S2RWNjs"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MSIhvTfCEjce"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvitTr9vEohJ",
        "outputId": "7a3b8a80-f6e2-4264-800f-b7de010cab1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSR29yoMErm3",
        "outputId": "65e65f26-4b97-4e4d-e6db-d0a9d58f954f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28', '29', '30', '31', '32']\n",
            "Data shape: (32, 40, 40, 8064)\n",
            "Labels shape: (32, 40, 4)\n"
          ]
        }
      ],
      "source": [
        "# Function to load data from each participant file\n",
        "def read_eeg_signal_from_file(filename):\n",
        "    with open(filename, 'rb') as file:\n",
        "        x = pickle._Unpickler(file)\n",
        "        x.encoding = 'latin1'\n",
        "        p = x.load()\n",
        "    return p\n",
        "\n",
        "# Load 32/32 participants\n",
        "files = [f'{n:02d}' for n in range(1, 33)]\n",
        "print(files)\n",
        "\n",
        "# 32x40 trials\n",
        "labels = []\n",
        "data = []\n",
        "\n",
        "for i in files:\n",
        "    filename = f\"/content/drive/MyDrive/DEAP/data_preprocessed_python/s{i}.dat\"\n",
        "    trial = read_eeg_signal_from_file(filename)\n",
        "    labels.append(trial['labels'])\n",
        "    data.append(trial['data'])\n",
        "\n",
        "labels = np.array(labels)\n",
        "data = np.array(data)\n",
        "\n",
        "print(\"Data shape:\", data.shape)  # (32, 40, 40, 8064) :: participant x trial x channel x data\n",
        "print(\"Labels shape:\", labels.shape)  # (32, 40, 4) :: participant x trial x label (valence, arousal, dominance, liking)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA-INITIALIZATION"
      ],
      "metadata": {
        "id": "b1oEf8iAYiYT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "80zSHWMLFBBK",
        "outputId": "d6ca5800-ec98-4278-87a0-62206aebc0c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Binary valence labels shape: (32, 40)\n"
          ]
        }
      ],
      "source": [
        "# Extracting VALENCE (for valence-based binary-classification)\n",
        "valence_labels = labels[:, :, 0]\n",
        "\n",
        "# Converting the valence labels into binary classes (0 or 1); DEAP recommends a threshold of 5\n",
        "binary_valence = (valence_labels > 5).astype(int)\n",
        "\n",
        "print(\"Binary valence labels shape:\", binary_valence.shape)  # Should be (32, 40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbYzKC5GFEMV",
        "outputId": "a02c9a30-704d-4826-81d0-837c135bcd8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reshaped data shape: (32, 40, 322560)\n"
          ]
        }
      ],
      "source": [
        "# Reshaping data: Flatten each trial (40 channels * 8064 time points = 322560 features per trial)\n",
        "reshaped_data = data.reshape(32, 40, -1)\n",
        "print(\"Reshaped data shape:\", reshaped_data.shape)  # (32, 40, 322560)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBO6wbgcFEgN",
        "outputId": "8ea379ba-dd27-46a6-de15-2a0f577b9f61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1024, 322560)\n",
            "Test data shape: (256, 322560)\n"
          ]
        }
      ],
      "source": [
        "# Flatten across all participants and trials\n",
        "X = reshaped_data.reshape(-1, reshaped_data.shape[-1])  # shape (32*40, 322560)\n",
        "y = binary_valence.flatten()  # shape (32*40, )\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Training data shape:\", X_train.shape)  # Should be (1024, 322560) if using 80% train\n",
        "print(\"Test data shape:\", X_test.shape)  # Should be (256, 322560)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####NORMALIZATION"
      ],
      "metadata": {
        "id": "DecqFWc-YqWC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "fLDlNb2cFI1W"
      },
      "outputs": [],
      "source": [
        "# Standardize the data (mean = 0, variance = 1)\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####PARAMETER INITIALIZATION FUNCTION\n",
        "\n",
        "*   Using He Weight Initialization for weights\n",
        "*   0 for biases"
      ],
      "metadata": {
        "id": "EYjGzie0QbYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initialize_parameters(input_size, hidden_size1, hidden_size2, output_size):\n",
        "    np.random.seed(1)\n",
        "\n",
        "    W1 = np.random.randn(hidden_size1, input_size) * np.sqrt(2./input_size)\n",
        "    b1 = np.zeros((hidden_size1, 1))\n",
        "\n",
        "    W2 = np.random.randn(hidden_size2, hidden_size1) * np.sqrt(2./hidden_size1)\n",
        "    b2 = np.zeros((hidden_size2, 1))\n",
        "\n",
        "    W3 = np.random.randn(output_size, hidden_size2) * np.sqrt(2./hidden_size2)\n",
        "    b3 = np.zeros((output_size, 1))\n",
        "\n",
        "    return W1, b1, W2, b2, W3, b3"
      ],
      "metadata": {
        "id": "ca0pPuoFQaN5"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####ACTIVATION FUNCTIONS"
      ],
      "metadata": {
        "id": "Z1wykYanPF7z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Leaky - ReLU"
      ],
      "metadata": {
        "id": "5MmI9n8wPz4m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def leaky_relu(Z, alpha=0.01):\n",
        "    return np.where(Z > 0, Z, alpha * Z)\n",
        "\n",
        "def leaky_relu_derivative(Z, alpha=0.01):\n",
        "    return np.where(Z > 0, 1, alpha)"
      ],
      "metadata": {
        "id": "pj-ad4EFQCa8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####Sigmoid"
      ],
      "metadata": {
        "id": "oSmUCXz3QDwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "    return 1 / (1 + np.exp(-Z))\n",
        "\n",
        "def sigmoid_derivative(Z):\n",
        "    return sigmoid(Z) * (1 - sigmoid(Z))"
      ],
      "metadata": {
        "id": "c5HIUisEQQ_U"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####FORWARD PROPAGATION"
      ],
      "metadata": {
        "id": "oRWLo-mBRstR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_propagation(X, W1, b1, W2, b2, W3, b3):\n",
        "\n",
        "    Z1 = np.dot(W1, X) + b1\n",
        "    A1 = leaky_relu(Z1)\n",
        "\n",
        "    Z2 = np.dot(W2, A1) + b2\n",
        "    A2 = leaky_relu(Z2)\n",
        "\n",
        "    Z3 = np.dot(W3, A2) + b3\n",
        "    A3 = sigmoid(Z3)\n",
        "\n",
        "    return Z1, A1, Z2, A2, Z3, A3"
      ],
      "metadata": {
        "id": "4_tw-z0pRwJB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####COST FUNCTION\n",
        "\n",
        "> Using binary cross-entropy"
      ],
      "metadata": {
        "id": "xwuZrwtaSF0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_cost(A3, Y):\n",
        "\n",
        "    m = Y.shape[1]\n",
        "    cost = (-1 / m) * np.sum(Y * np.log(A3 + 1e-15) + (1 - Y) * np.log(1 - A3 + 1e-15))\n",
        "\n",
        "    return np.squeeze(cost)"
      ],
      "metadata": {
        "id": "d2cFfEAjSJVl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####BACKWARD PROPAGATION"
      ],
      "metadata": {
        "id": "y8JljxUtSdqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def backward_propagation(X, Y, Z1, A1, Z2, A2, Z3, A3, W2, W3):\n",
        "\n",
        "    m = X.shape[1]\n",
        "\n",
        "    # Output layer\n",
        "    dZ3 = A3 - Y\n",
        "    dW3 = (1 / m) * np.dot(dZ3, A2.T)\n",
        "    db3 = (1 / m) * np.sum(dZ3, axis=1, keepdims=True)\n",
        "\n",
        "    # Second hidden layer\n",
        "    dA2 = np.dot(W3.T, dZ3)\n",
        "    dZ2 = dA2 * leaky_relu_derivative(Z2)\n",
        "    dW2 = (1 / m) * np.dot(dZ2, A1.T)\n",
        "    db2 = (1 / m) * np.sum(dZ2, axis=1, keepdims=True)\n",
        "\n",
        "    # First hidden layer\n",
        "    dA1 = np.dot(W2.T, dZ2)\n",
        "    dZ1 = dA1 * leaky_relu_derivative(Z1)\n",
        "    dW1 = (1 / m) * np.dot(dZ1, X.T)\n",
        "    db1 = (1 / m) * np.sum(dZ1, axis=1, keepdims=True)\n",
        "\n",
        "    return dW1, db1, dW2, db2, dW3, db3"
      ],
      "metadata": {
        "id": "HtWHnsu9ShCW"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####GRADIENT DESCENT & PREDICT FUNCTION"
      ],
      "metadata": {
        "id": "oEP2BKt0Syox"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "74nQB76BSNlq"
      },
      "outputs": [],
      "source": [
        "def update_parameters(W1, b1, W2, b2, W3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate):\n",
        "    W1 -= learning_rate * dW1\n",
        "    b1 -= learning_rate * db1\n",
        "    W2 -= learning_rate * dW2\n",
        "    b2 -= learning_rate * db2\n",
        "    W3 -= learning_rate * dW3\n",
        "    b3 -= learning_rate * db3\n",
        "    return W1, b1, W2, b2, W3, b3\n",
        "\n",
        "def predict(X, W1, b1, W2, b2, W3, b3):\n",
        "    _, _, _, _, _, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
        "    predictions = (A3 > 0.5).astype(int)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####DEEP NEURAL NETWORK"
      ],
      "metadata": {
        "id": "-yz1IuP7UJht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def neural_network(X, Y, hidden_size1, hidden_size2, epochs, learning_rate):\n",
        "\n",
        "    input_size = X.shape[0]\n",
        "    output_size = 1\n",
        "\n",
        "    # Initialization\n",
        "    W1, b1, W2, b2, W3, b3 = initialize_parameters(input_size, hidden_size1, hidden_size2, output_size)\n",
        "\n",
        "    costs = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        # Forward propagation\n",
        "        Z1, A1, Z2, A2, Z3, A3 = forward_propagation(X, W1, b1, W2, b2, W3, b3)\n",
        "\n",
        "        # Cost\n",
        "        cost = compute_cost(A3, Y)\n",
        "        costs.append(cost)\n",
        "\n",
        "        # Backward propagation\n",
        "        dW1, db1, dW2, db2, dW3, db3 = backward_propagation(X, Y, Z1, A1, Z2, A2, Z3, A3, W2, W3)\n",
        "\n",
        "        # Parameters update\n",
        "        W1, b1, W2, b2, W3, b3 = update_parameters(\n",
        "            W1, b1, W2, b2, W3, b3,\n",
        "            dW1, db1, dW2, db2, dW3, db3,\n",
        "            learning_rate\n",
        "        )\n",
        "\n",
        "        # Accuracy calculation\n",
        "        predictions = (A3 > 0.5).astype(int)\n",
        "        accuracy = np.mean(predictions == Y) * 100\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Cost: {cost:.4f}, Training Accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "    # Cost vs Epochs plot\n",
        "    plt.plot(costs)\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Cost')\n",
        "    plt.title('Cost vs Epochs')\n",
        "    plt.show()\n",
        "\n",
        "    return W1, b1, W2, b2, W3, b3"
      ],
      "metadata": {
        "id": "hMtplYUPUH6a"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "lDTu0VRzUxkL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_size1 = 128                          # Size of first hidden layer\n",
        "hidden_size2 = 64                           # Size of second hidden layer\n",
        "epochs = 100\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Train\n",
        "W1, b1, W2, b2, W3, b3 = neural_network(\n",
        "    X_train.T,\n",
        "    y_train.reshape(1, -1),\n",
        "    hidden_size1,\n",
        "    hidden_size2,\n",
        "    epochs,\n",
        "    learning_rate\n",
        ")\n",
        "\n",
        "# Predict\n",
        "predictions = predict(X_test.T, W1, b1, W2, b2, W3, b3)\n",
        "accuracy = np.mean(predictions == y_test.reshape(1, -1)) * 100\n",
        "\n",
        "print(f\"Test set accuracy: {accuracy}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1JZDEPW4N_k1",
        "outputId": "74df01c0-9ccb-4c55-ce83-ac2641c036fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, Cost: 0.8950, Training Accuracy: 53.12%\n",
            "Epoch 10, Cost: 0.4264, Training Accuracy: 82.42%\n",
            "Epoch 20, Cost: 0.3200, Training Accuracy: 93.46%\n",
            "Epoch 30, Cost: 0.2602, Training Accuracy: 97.27%\n",
            "Epoch 40, Cost: 0.2206, Training Accuracy: 98.93%\n",
            "Epoch 50, Cost: 0.1920, Training Accuracy: 99.61%\n",
            "Epoch 60, Cost: 0.1701, Training Accuracy: 99.71%\n",
            "Epoch 70, Cost: 0.1527, Training Accuracy: 99.90%\n",
            "Epoch 80, Cost: 0.1386, Training Accuracy: 99.90%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Scatter plot; picking first two of 322,560 features\n",
        "feature_x = X[:, 0]\n",
        "feature_y = X[:, 1]\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.scatterplot(x=feature_x, y=feature_y, hue=y, palette=\"coolwarm\", style=y, s=100)\n",
        "\n",
        "plt.title(\"Scatter Plot of Feature 1 vs Feature 2 based on Binary Valence\", fontsize=14)\n",
        "plt.xlabel(\"Feature 1\", fontsize=12)\n",
        "plt.ylabel(\"Feature 2\", fontsize=12)\n",
        "plt.legend(title=\"Valence (0 or 1)\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "likkZhX9IhwS"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPOPvh+6IUJSbP5xhdjNCFh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}